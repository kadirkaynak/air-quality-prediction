---------Experiment Parameters--------
Past feature window: 10
Future feature window: 12
Train-Test split rate: 0.8
Number of epochs: 100
Batch size: 60
Shuffle during training: True
Verbose: 0
---------------------------
---------Dataset Details--------
No. of sensors:207
No of timesteps:2016
Train data: (207, 1612)
Test data: (207, 404)
Train data X: (1591, 207, 10)
Train data Y: (1591, 207)
Test data X: (383, 207, 10)
Test data Y: (383, 207)
---------------------------
---------Model Details--------
GCN layer sizes: [16, 10]
GCN activation functions: ['relu', 'relu']
LSTM layer sizes: [200, 200]
LSTM activation functions: ['tanh', 'tanh']
Optimizer: adam
Loss function: mae
Result Metrics: ['mse']
---------------------------
Last Train loss: 0.055966295301914215, Last Test loss:0.06445811688899994
Total (ave) MAE for NN: 4.32044353758169
Total (ave) MAE for naive prediction: 5.619645381284217
Total (ave) MASE for per-segment NN/naive MAE: 0.7568343770285832
...note that MASE<1 (for a given segment) means that theNN prediction is better than the naive prediction.