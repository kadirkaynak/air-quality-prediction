{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b062717",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import locale\n",
    "from locale import atof\n",
    "import xlsxwriter\n",
    "\n",
    "from PreProcessingUtil import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1028e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Tarih PM10 ( µg/m3 ) SO2 ( µg/m3 ) CO ( µg/m3 )  \\\n",
      "0     2012-01-01 01:00:56              -             -            -   \n",
      "1     2012-01-01 02:00:56              -             -            -   \n",
      "2     2012-01-01 03:00:56              -             -            -   \n",
      "3     2012-01-01 04:00:56              -             -            -   \n",
      "4     2012-01-01 05:00:56              -             -            -   \n",
      "...                   ...            ...           ...          ...   \n",
      "81690 2021-04-26 19:00:56          11,10          1,00       296,70   \n",
      "81691 2021-04-26 20:00:56          17,40          0,80       254,10   \n",
      "81692 2021-04-26 21:00:56          30,00          0,70       180,00   \n",
      "81693 2021-04-26 22:00:56          18,60          1,00       399,00   \n",
      "81694 2021-04-26 23:00:56              -          1,10       446,60   \n",
      "\n",
      "      NO2 ( µg/m3 ) NOX ( µg/m3 ) O3 ( µg/m3 ) PM 2.5 ( µg/m3 )  \n",
      "0                 -             -            -                -  \n",
      "1                 -             -            -                -  \n",
      "2                 -             -            -                -  \n",
      "3                 -             -            -                -  \n",
      "4                 -             -            -                -  \n",
      "...             ...           ...          ...              ...  \n",
      "81690         15,90             -        31,60                -  \n",
      "81691         15,80             -        34,40                -  \n",
      "81692         18,90             -        31,70                -  \n",
      "81693         23,30             -        17,10                -  \n",
      "81694         22,10             -        22,20                -  \n",
      "\n",
      "[81695 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\n",
    "     os.path.join(\"../datasets\", \"pollutants\", \"besiktas.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True,\n",
    "     thousands='.'\n",
    ")\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f48522a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '-' string with NaN\n",
    "df = df.replace ('-', '-1')\n",
    "\n",
    "# Also fixing ',' delimeter with '.' for float conversion\n",
    "# . is for thousands , for the last delimeter\n",
    "df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "\n",
    "# Casting str to float\n",
    "df['PM10 ( µg/m3 )'] = pd.to_numeric(df['PM10 ( µg/m3 )'], downcast=\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8a1f85ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tarih                   0\n",
       "PM10 ( µg/m3 )          0\n",
       "SO2 ( µg/m3 )       61745\n",
       "CO ( µg/m3 )        62154\n",
       "NO2 ( µg/m3 )       63773\n",
       "NOX ( µg/m3 )       67718\n",
       "O3 ( µg/m3 )        62743\n",
       "PM 2.5 ( µg/m3 )    67638\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting number of -1 in the column\n",
    "df[df == '-1'].count () "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b29120fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling empty rows\n",
    "# https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e\n",
    "# 1- We cant just delete them because we need consistent timestamps\n",
    "# 2- If too many empty rows exists we should discard them\n",
    "# 3- Replacing missing data with mean/median\n",
    "# 3.1- This does not cover the covariance between features\n",
    "df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].replace (-1.0, df[df != -1]['PM10 ( µg/m3 )'].median ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1dc3118f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        26.1\n",
       "1        26.1\n",
       "2        26.1\n",
       "3        26.1\n",
       "4        26.1\n",
       "         ... \n",
       "81690    11.1\n",
       "81691    17.4\n",
       "81692    30.0\n",
       "81693    18.6\n",
       "81694    26.1\n",
       "Name: PM10 ( µg/m3 ), Length: 81695, dtype: float32"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PM10 ( µg/m3 )']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbc0e57",
   "metadata": {},
   "source": [
    "## Town - Pollution Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e90e8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes per town\n",
    "dfAksaray = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"aksaray.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfBesiktas = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"besiktas.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfBuyukada = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"buyukada.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfCatladıkapı = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"catladıkapı.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfEsenler = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"esenler.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfKadıkoy = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"kadıkoy.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfKandilli = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"kandilli.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfKartal = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"kartal.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfMecidiyekoy = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"mecidiyekoy.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')\n",
    "dfUmraniye = pd.read_excel(os.path.join(\"../\", \"datasets\", \"pollutants\", \"umraniye.xlsx\"),engine='openpyxl',parse_dates=True,thousands='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c518f01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fixValues(df):\n",
    "    # Replace '-' string with NaN\n",
    "    df = df.replace ('-', '-1')\n",
    "\n",
    "    # Also fixing ',' delimeter with '.' for float conversion '.' is for thousands , for the last delimeter\n",
    "    # Casting str to float\n",
    "    df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "    df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "    df['PM10 ( µg/m3 )'] = pd.to_numeric(df['PM10 ( µg/m3 )'], downcast=\"float\")\n",
    "    \n",
    "    df['SO2 ( µg/m3 )'] = df['SO2 ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "    df['SO2 ( µg/m3 )'] = df['SO2 ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "    df['SO2 ( µg/m3 )'] = pd.to_numeric(df['SO2 ( µg/m3 )'], downcast=\"float\")\n",
    "    \n",
    "    df['CO ( µg/m3 )'] = df['CO ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "    df['CO ( µg/m3 )'] = df['CO ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "    df['CO ( µg/m3 )'] = pd.to_numeric(df['CO ( µg/m3 )'], downcast=\"float\")\n",
    "    \n",
    "    df['NO2 ( µg/m3 )'] = df['NO2 ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "    df['NO2 ( µg/m3 )'] = df['NO2 ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "    df['NO2 ( µg/m3 )'] = pd.to_numeric(df['NO2 ( µg/m3 )'], downcast=\"float\")\n",
    "    \n",
    "    df['NOX ( µg/m3 )'] = df['NOX ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "    df['NOX ( µg/m3 )'] = df['NOX ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "    df['NOX ( µg/m3 )'] = pd.to_numeric(df['NOX ( µg/m3 )'], downcast=\"float\")\n",
    "    \n",
    "    df['O3 ( µg/m3 )'] = df['O3 ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "    df['O3 ( µg/m3 )'] = df['O3 ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "    df['O3 ( µg/m3 )'] = pd.to_numeric(df['O3 ( µg/m3 )'], downcast=\"float\")\n",
    "    \n",
    "    df['PM 2.5 ( µg/m3 )'] = df['PM 2.5 ( µg/m3 )'].astype(str).str.replace('.','')\n",
    "    df['PM 2.5 ( µg/m3 )'] = df['PM 2.5 ( µg/m3 )'].astype(str).str.replace(',','.')\n",
    "    df['PM 2.5 ( µg/m3 )'] = pd.to_numeric(df['PM 2.5 ( µg/m3 )'], downcast=\"float\")\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def fillEmptyRows(df):\n",
    "    # Filling empty rows\n",
    "    # https://towardsdatascience.com/7-ways-to-handle-missing-values-in-machine-learning-1a6326adf79e\n",
    "    # 1- We cant just delete them because we need consistent timestamps\n",
    "    # 2- If too many empty rows exists we should discard them\n",
    "    # 3- Replacing missing data with mean/median\n",
    "    # 3.1- This does not cover the covariance between features\n",
    "    \n",
    "    #df['PM10 ( µg/m3 )'] = df['PM10 ( µg/m3 )'].replace (-1.0, df[df != -1]['PM10 ( µg/m3 )'].median ())\n",
    "    #df['SO2 ( µg/m3 )'] = df['SO2 ( µg/m3 )'].replace (-1.0, df[df != -1]['SO2 ( µg/m3 )'].median ())\n",
    "    #df['CO ( µg/m3 )'] = df['CO ( µg/m3 )'].replace (-1.0, df[df != -1]['CO ( µg/m3 )'].median ())\n",
    "    #df['NO2 ( µg/m3 )'] = df['NO2 ( µg/m3 )'].replace (-1.0, df[df != -1]['NO2 ( µg/m3 )'].median ())\n",
    "    #df['NOX ( µg/m3 )'] = df['NOX ( µg/m3 )'].replace (-1.0, df[df != -1]['NOX ( µg/m3 )'].median ())\n",
    "    #df['O3 ( µg/m3 )'] = df['O3 ( µg/m3 )'].replace (-1.0, df[df != -1]['O3 ( µg/m3 )'].median ())\n",
    "    #df['PM 2.5 ( µg/m3 )'] = df['PM 2.5 ( µg/m3 )'].replace (-1.0, df[df != -1]['PM 2.5 ( µg/m3 )'].median ())\n",
    "    if not(df['PM10 ( µg/m3 )'][df['PM10 ( µg/m3 )']!=-1].count()>0):df['PM10 ( µg/m3 )']=0\n",
    "    if not(df['SO2 ( µg/m3 )'][df['SO2 ( µg/m3 )']!=-1].count()>0):df['SO2 ( µg/m3 )']=0\n",
    "    if not(df['CO ( µg/m3 )'][df['CO ( µg/m3 )']!=-1].count()>0):df['CO ( µg/m3 )']=0\n",
    "    if not(df['NO2 ( µg/m3 )'][df['NO2 ( µg/m3 )']!=-1].count()>0):df['NO2 ( µg/m3 )']=0\n",
    "    if not(df['NOX ( µg/m3 )'][df['NOX ( µg/m3 )']!=-1].count()>0):df['NOX ( µg/m3 )']=0\n",
    "    if not(df['O3 ( µg/m3 )'][df['O3 ( µg/m3 )']!=-1].count()>0):df['O3 ( µg/m3 )']=0\n",
    "    if not(df['PM 2.5 ( µg/m3 )'][df['PM 2.5 ( µg/m3 )']!=-1].count()>0):df['PM 2.5 ( µg/m3 )']=0\n",
    "\n",
    "    #if not(df['PM10 ( µg/m3 )'].notnull().values.any()): df['PM10 ( µg/m3 )']=0\n",
    "    #if not(df['SO2 ( µg/m3 )'].notnull().values.any()): df['SO2 ( µg/m3 )']=0\n",
    "    #if not(df['CO ( µg/m3 )'].notnull().values.any()): df['CO ( µg/m3 )']=0\n",
    "    #if not(df['NO2 ( µg/m3 )'].notnull().values.any()): df['NO2 ( µg/m3 )']=0\n",
    "    #if not(df['NOX ( µg/m3 )'].notnull().values.any()): df['NOX ( µg/m3 )']=0\n",
    "    #if not(df['O3 ( µg/m3 )'].notnull().values.any()): df['O3 ( µg/m3 )']=0\n",
    "    #if not(df['PM 2.5 ( µg/m3 )'].notnull().values.any()): df['PM 2.5 ( µg/m3 )']=0\n",
    "\n",
    "    df = fillWithSameHourValue(df, 'PM10 ( µg/m3 )')\n",
    "    df = fillWithSameHourValue(df, 'SO2 ( µg/m3 )')\n",
    "    df = fillWithSameHourValue(df, 'CO ( µg/m3 )')\n",
    "    df = fillWithSameHourValue(df, 'NO2 ( µg/m3 )')\n",
    "    df = fillWithSameHourValue(df, 'NOX ( µg/m3 )')\n",
    "    df = fillWithSameHourValue(df, 'O3 ( µg/m3 )')\n",
    "    df = fillWithSameHourValue(df, 'PM 2.5 ( µg/m3 )')\n",
    "    return df\n",
    "\n",
    "def fillWithSameHourValue(df, pollutantColumnName):\n",
    "    for x in range (df[pollutantColumnName].shape[0]):\n",
    "        if(df[pollutantColumnName].iloc[x] == -1 and x < 24):\n",
    "            df[pollutantColumnName].iloc[x] = findValue(df[pollutantColumnName], x)\n",
    "        if(df[pollutantColumnName].iloc[x] == -1 and x >= 24):\n",
    "            df[pollutantColumnName].iloc[x] = df[pollutantColumnName].iloc[x-24]\n",
    "        \n",
    "    return df\n",
    "\n",
    "def findValue(df, i):\n",
    "    if(df.shape[0]<=i):\n",
    "        return 0\n",
    "\n",
    "    if(df.iloc[i] == -1):\n",
    "        i = i+24\n",
    "        return findValue(df, i)\n",
    "    else:\n",
    "        return df.iloc[i] \n",
    "\n",
    "def preprocessingx(df):\n",
    "    df = fixValues(df)\n",
    "    df = fillEmptyRows(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e49f226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n",
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# making preprocessing (fixing values, filling empty rows)\n",
    "dfAksaray = preprocessing(dfAksaray)\n",
    "dfBesiktas = preprocessing(dfBesiktas)\n",
    "dfBuyukada = preprocessing(dfBuyukada)\n",
    "dfCatladıkapı = preprocessing(dfCatladıkapı)\n",
    "dfEsenler = preprocessing(dfEsenler)\n",
    "dfKadıkoy = preprocessing(dfKadıkoy)\n",
    "dfKandilli = preprocessing(dfKandilli)\n",
    "dfKartal = preprocessing(dfKartal)\n",
    "dfMecidiyekoy = preprocessing(dfMecidiyekoy)\n",
    "dfUmraniye = preprocessing(dfUmraniye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3aad4e22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Aksaray    Besiktas  Buyukada  Catladıkapı     Esenler     Kadıkoy  \\\n",
      "0       65.300003   92.400002         0    49.099998   97.400002   35.099998   \n",
      "1       55.500000   86.699997         0    64.599998  100.300003   30.600000   \n",
      "2       75.500000  108.400002         0    81.000000  106.599998   30.299999   \n",
      "3       83.000000  133.500000         0    95.300003  119.000000   39.200001   \n",
      "4       98.500000  129.000000         0   102.099998  128.199997   35.299999   \n",
      "...           ...         ...       ...          ...         ...         ...   \n",
      "19995  300.399994   15.900000         0    98.199997   64.000000   61.700001   \n",
      "19996  322.500000   15.800000         0   125.099998   84.099998   74.300003   \n",
      "19997  318.500000   18.900000         0    98.900002   99.199997   73.500000   \n",
      "19998  317.700012   23.299999         0    73.900002  101.199997   98.900002   \n",
      "19999  309.200012   22.100000         0    99.400002  109.400002  104.500000   \n",
      "\n",
      "       Kandilli     Kartal  Mecidiyekoy    Umraniye  \n",
      "0             0  53.799999    60.660000   37.400002  \n",
      "1             0  63.299999    70.900002   40.299999  \n",
      "2             0  70.000000    69.800003   46.400002  \n",
      "3             0  72.400002    78.150002   57.799999  \n",
      "4             0  83.300003    80.349998   69.300003  \n",
      "...         ...        ...          ...         ...  \n",
      "19995         0  70.800003    68.370003  158.699997  \n",
      "19996         0  88.000000    75.269997  180.000000  \n",
      "19997         0  77.000000    66.680000  178.300003  \n",
      "19998         0  73.400002    71.750000  147.300003  \n",
      "19999         0  97.699997    65.839996  217.100006  \n",
      "\n",
      "[20000 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize data to Dicts of series.  \n",
    "lastNItems = 20000\n",
    "columnName = 'NO2 ( µg/m3 )'\n",
    "excelName = '../datasets/training/NO2.xlsx'\n",
    "\n",
    "d = {\n",
    "    'Aksaray' : dfAksaray[columnName][-lastNItems:].tolist(),  \n",
    "    'Besiktas' : dfBesiktas[columnName][-lastNItems:].tolist(),\n",
    "    'Buyukada' : dfBuyukada[columnName][-lastNItems:].tolist(), \n",
    "    'Catladıkapı' : dfCatladıkapı[columnName][-lastNItems:].tolist(),\n",
    "    'Esenler' : dfEsenler[columnName][-lastNItems:].tolist(), \n",
    "    'Kadıkoy' : dfKadıkoy[columnName][-lastNItems:].tolist(),\n",
    "    'Kandilli' : dfKandilli[columnName][-lastNItems:].tolist(), \n",
    "    'Kartal'    : dfKartal[columnName][-lastNItems:].tolist(),\n",
    "    'Mecidiyekoy' : dfMecidiyekoy[columnName][-lastNItems:].tolist(), \n",
    "    'Umraniye'    : dfUmraniye[columnName][-lastNItems:].tolist()\n",
    "    \n",
    "}  \n",
    "  \n",
    "# creates Dataframe.  \n",
    "dframe = pd.DataFrame(d, columns = ['Aksaray', 'Besiktas','Buyukada','Catladıkapı','Esenler','Kadıkoy','Kandilli','Kartal','Mecidiyekoy','Umraniye']) \n",
    "  \n",
    "# print the data.  \n",
    "print(dframe) \n",
    "\n",
    "# export to excel\n",
    "dframe.to_excel(excelName) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857bf23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pollution Diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5afec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sehirList = ['Aksaray', 'Besiktas','Buyukada','Catladıkapı','Esenler','Kadıkoy','Kandilli','Kartal','Mecidiyekoy','Umraniye']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e813f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sensors = len(sehirList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe['Aksaray']-dframe['Besiktas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2875d103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing adjacency matrix for the sensor nodes\n",
    "pol_diff_matrix = np.zeros ((n_sensors, n_sensors))\n",
    "\n",
    "# Create distance matrix from each sensor to other ones\n",
    "# For each sensor\n",
    "ix = 0\n",
    "for s_sehir in sehirList:\n",
    "    s = dframe[s_sehir]\n",
    "    # Look at other sensors\n",
    "    o_ix = 0\n",
    "    for o_sehir in sehirList:\n",
    "        o = dframe[o_sehir]\n",
    "        # Calculate the distance\n",
    "        dframe['diff'] = s-o\n",
    "        # Update the distance matrix\n",
    "        pol_diff_matrix [ix][o_ix] = abs(dframe['diff'].mean())\n",
    "        #\n",
    "        o_ix += 1\n",
    "    ix += 1\n",
    "    \n",
    "# Take the absolute of the difference between all values and the max value\n",
    "# Then divide it by the max value to get the weighted adjacency matrix\n",
    "# Add 1 to the max value so that the weights will never be lost\n",
    "max_val = np.max (pol_diff_matrix) + 1\n",
    "pol_diff_matrix = (max_val - pol_diff_matrix)/max_val\n",
    "pol_diff_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('datasets/training/pol_diff_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(pol_diff_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e82d28",
   "metadata": {},
   "source": [
    "## Sensor Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "311e2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "# Distance between two coordinates taken from\n",
    "# https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n",
    "def coord_distance(lat1, lon1, lat2, lon2):\n",
    "    p = pi/180\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 12742 * asin(sqrt(a)) #2*R*asin..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19f12573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          İlçe    Enlem   Boylam\n",
      "0      aksaray  41.0244  29.0997\n",
      "1     besiktas  41.0520  29.0094\n",
      "2     buyukada  40.8521  29.1180\n",
      "3  catladıkapı  41.0023  28.9751\n",
      "4      esenler  41.0368  28.8880\n",
      "5      kadıkoy  40.9908  29.0333\n",
      "6     kandilli  41.0624  29.0582\n",
      "7       kartal  40.9110  29.1830\n",
      "8  mecidiyekoy  41.0659  28.9944\n",
      "9     umraniye  41.0126  29.1618\n",
      "(10, 3)\n"
     ]
    }
   ],
   "source": [
    "sensor_l = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/sensor_locations.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True,\n",
    ")\n",
    "n_sensors = sensor_l.shape[0]\n",
    "print (sensor_l)\n",
    "print (sensor_l.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0326e776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.72246006, 0.34720973, 0.63525996, 0.39507831,\n",
       "        0.77216223, 0.81407726, 0.51027434, 0.66159891, 0.81752721],\n",
       "       [0.72246006, 1.        , 0.18399078, 0.78839024, 0.64948686,\n",
       "        0.75905202, 0.85558757, 0.27301488, 0.93232307, 0.54105276],\n",
       "       [0.34720973, 0.18399078, 1.        , 0.30142131, 0.04239443,\n",
       "        0.42311837, 0.18768574, 0.71030258, 0.11898075, 0.38111595],\n",
       "       [0.63525996, 0.78839024, 0.30142131, 1.        , 0.71969965,\n",
       "        0.828519  , 0.67203794, 0.31409166, 0.75359924, 0.46650847],\n",
       "       [0.39507831, 0.64948686, 0.04239443, 0.71969965, 1.        ,\n",
       "        0.5509857 , 0.50571456, 0.03396331, 0.67766267, 0.21457354],\n",
       "       [0.77216223, 0.75905202, 0.42311837, 0.828519  , 0.5509857 ,\n",
       "        1.        , 0.72044807, 0.47736705, 0.6954975 , 0.62461969],\n",
       "       [0.81407726, 0.85558757, 0.18768574, 0.67203794, 0.50571456,\n",
       "        0.72044807, 1.        , 0.32657829, 0.8178543 , 0.65005373],\n",
       "       [0.51027434, 0.27301488, 0.71030258, 0.31409166, 0.03396331,\n",
       "        0.47736705, 0.32657829, 1.        , 0.20547643, 0.61156863],\n",
       "       [0.66159891, 0.93232307, 0.11898075, 0.75359924, 0.67766267,\n",
       "        0.6954975 , 0.8178543 , 0.20547643, 1.        , 0.48241643],\n",
       "       [0.81752721, 0.54105276, 0.38111595, 0.46650847, 0.21457354,\n",
       "        0.62461969, 0.65005373, 0.61156863, 0.48241643, 1.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing adjacency matrix for the sensor nodes\n",
    "distance_matrix = np.zeros ((n_sensors, n_sensors))\n",
    "\n",
    "# Create distance matrix from each sensor to other ones\n",
    "# For each sensor\n",
    "ix = 0\n",
    "for sensor in sensor_l['İlçe']:\n",
    "    s_lat = sensor_l[sensor_l['İlçe'] == sensor].values[0][1]\n",
    "    s_lon = sensor_l[sensor_l['İlçe'] == sensor].values[0][2]\n",
    "    # Look at other sensors\n",
    "    o_ix = 0\n",
    "    for o_sensor in sensor_l['İlçe']:\n",
    "        o_lat = sensor_l[sensor_l['İlçe'] == o_sensor].values[0][1] \n",
    "        o_lon = sensor_l[sensor_l['İlçe'] == o_sensor].values[0][2] \n",
    "        # Calculate the distance\n",
    "        distance = coord_distance (s_lat, s_lon, o_lat, o_lon)\n",
    "        # Update the distance matrix\n",
    "        distance_matrix [ix][o_ix] = distance\n",
    "        #\n",
    "        o_ix += 1\n",
    "    ix += 1\n",
    "    \n",
    "# Take the absolute of the difference between all values and the max value\n",
    "# Then divide it by the max value to get the weighted adjacency matrix\n",
    "# Add 1 to the max value so that the weights will never be lost\n",
    "max_val = np.max (distance_matrix) + 1\n",
    "distance_matrix = (max_val - distance_matrix)/max_val\n",
    "distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74783033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('datasets/training/sensor_dist_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(distance_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4e520",
   "metadata": {},
   "source": [
    "## Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aa56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_l = pd.read_excel(\n",
    "     os.path.join(\"datasets/adjacency/population.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True,\n",
    ")\n",
    "n_sensors = sensor_l.shape[0]\n",
    "print (sensor_l)\n",
    "print (sensor_l.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bbb62",
   "metadata": {},
   "source": [
    "## Randomized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1791ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create uniform random matrix with values between 0 and 1\n",
    "n_cities = 10\n",
    "randomized_matrix = np.random.rand (n_cities, n_cities)\n",
    "\n",
    "# Make diagonal 1 because they are themselves\n",
    "np.fill_diagonal(randomized_matrix, 1)\n",
    "\n",
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/random_dist_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(randomized_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50e28bc",
   "metadata": {},
   "source": [
    "## Gas Usage For Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d32bd3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For natural gas usage NO2 and SO2 pollutants are important\n",
    "gas_pd = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/natural_gas_consumption.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True,\n",
    "     sheet_name='Tüketim verileri(m3)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05ff6488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İlçe listesi\n",
    "sehirList = ['FATİH', 'BEŞİKTAŞ','ADALAR','FATİH','ESENLER','KADIKÖY','ÜSKÜDAR','KARTAL','ŞİŞLİ','ÜMRANİYE']\n",
    "\n",
    "# Preparing adjacency matrix for the sensor nodes\n",
    "gas_matrix = np.zeros ((n_sensors, n_sensors))\n",
    "\n",
    "# Create custom dictionary in order to create a new pandas dataframe for correlation\n",
    "district_gas_dict = {}\n",
    "\n",
    "for data in gas_pd['İlçe']:\n",
    "    if data in sehirList:\n",
    "        row = gas_pd.loc[gas_pd['İlçe'] == data]\n",
    "        gas_val = row[2019].values[0]\n",
    "        \n",
    "        district_gas_dict [data] = gas_val\n",
    "        \n",
    "final_dict = []        \n",
    "        \n",
    "for dist in sehirList:\n",
    "    final_dict.append( district_gas_dict [dist])\n",
    "    \n",
    "final_dict = np.asarray (final_dict)\n",
    "final_dict = final_dict / final_dict.max ()\n",
    "\n",
    "# We take the difference between consumptions\n",
    "for ix_one, sehir in enumerate (sehirList):\n",
    "    for ix_two, sehir in enumerate (sehirList):\n",
    "        if ix_one == ix_two:\n",
    "            gas_matrix [ix_one, ix_two] = 1\n",
    "        else:\n",
    "            gas_matrix [ix_one, ix_two] = np.abs (final_dict [ix_one] - final_dict [ix_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "396a7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/gas_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(gas_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc223cf",
   "metadata": {},
   "source": [
    "## Waste Processing Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22cb2779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For natural gas usage NO2 and SO2 pollutants are important\n",
    "waste_pd = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/waste_facility.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e77d52a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sensor locations sensor_l, calculate the sum of distance from each waste facility to sensors\n",
    "waste_dist = []\n",
    "for s_l in sensor_l ['İlçe']:\n",
    "    # Get current sensors location data\n",
    "    s_l_data = sensor_l [sensor_l ['İlçe'] == s_l]\n",
    "    sensor_lat = s_l_data ['Enlem'].values [0]\n",
    "    sensor_lon = s_l_data ['Boylam'].values [0]\n",
    "    total_distance = 0\n",
    "    # Iterate all waste faciliies\n",
    "    for index, row in waste_pd.iterrows():\n",
    "        waste_lat = row ['LATITUDE']\n",
    "        waste_lon = row ['LONGTITUDE']\n",
    "        # Calculate distance\n",
    "        dist = coord_distance (sensor_lat, sensor_lon, waste_lat, waste_lon)\n",
    "        total_distance += dist\n",
    "    \n",
    "    waste_dist.append (total_distance)\n",
    "    \n",
    "# Calculate the adj matrix, the distance is inversly correlated so higher distance means lower pollution\n",
    "# from the facilities\n",
    "waste_dist = np.asarray (waste_dist)\n",
    "waste_dist = 1 / waste_dist\n",
    "waste_dist = waste_dist / waste_dist.max ()\n",
    "\n",
    "waste_adj = np.zeros ((len(waste_dist), len (waste_dist)))\n",
    "\n",
    "# We take the difference between consumptions\n",
    "for ix_one, waste in enumerate (waste_dist):\n",
    "    for ix_two, waste in enumerate (waste_dist):\n",
    "        if ix_one == ix_two:\n",
    "            waste_adj [ix_one, ix_two] = 1\n",
    "        else:\n",
    "            waste_adj [ix_one, ix_two] = np.abs (waste_dist [ix_one] - waste_dist [ix_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7f1e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/waste_facilities_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(waste_adj):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd796b5",
   "metadata": {},
   "source": [
    "## Park Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d14c658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>NEIGHBORHOOD_NAME</th>\n",
       "      <th>COUNTY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yayla Sokak Parkı</td>\n",
       "      <td>28.925082</td>\n",
       "      <td>41.081198</td>\n",
       "      <td>AKŞEMSETTİN</td>\n",
       "      <td>EYÜPSULTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stad Parkı</td>\n",
       "      <td>28.914355</td>\n",
       "      <td>40.994647</td>\n",
       "      <td>TELSİZ</td>\n",
       "      <td>ZEYTİNBURNU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kadriye Gök Parkı</td>\n",
       "      <td>28.899872</td>\n",
       "      <td>40.990061</td>\n",
       "      <td>NURİPAŞA</td>\n",
       "      <td>ZEYTİNBURNU</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Çukurbostan Parkı</td>\n",
       "      <td>28.933638</td>\n",
       "      <td>41.011341</td>\n",
       "      <td>ŞEHREMİNİ</td>\n",
       "      <td>FATİH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tutya Parkı</td>\n",
       "      <td>28.937360</td>\n",
       "      <td>41.009753</td>\n",
       "      <td>SEYYİD ÖMER</td>\n",
       "      <td>FATİH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>1415. Sok . Parkı</td>\n",
       "      <td>28.895567</td>\n",
       "      <td>41.104201</td>\n",
       "      <td>GAZİ</td>\n",
       "      <td>SULTANGAZİ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3623</th>\n",
       "      <td>Yedikule Surları Parkı</td>\n",
       "      <td>28.922778</td>\n",
       "      <td>40.993955</td>\n",
       "      <td>YEDİKULE</td>\n",
       "      <td>FATİH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Bağlarbaşı Çocuk Parkı</td>\n",
       "      <td>29.035293</td>\n",
       "      <td>41.024379</td>\n",
       "      <td>SELAMİ ALİ</td>\n",
       "      <td>ÜSKÜDAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3625</th>\n",
       "      <td>Park</td>\n",
       "      <td>29.027359</td>\n",
       "      <td>41.085210</td>\n",
       "      <td>AKAT</td>\n",
       "      <td>BEŞİKTAŞ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3626</th>\n",
       "      <td>Park</td>\n",
       "      <td>28.827317</td>\n",
       "      <td>41.007614</td>\n",
       "      <td>ÇOBANÇEŞME</td>\n",
       "      <td>BAHÇELİEVLER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3627 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        NAME  LONGITUDE   LATITUDE NEIGHBORHOOD_NAME  \\\n",
       "0          Yayla Sokak Parkı  28.925082  41.081198       AKŞEMSETTİN   \n",
       "1                 Stad Parkı  28.914355  40.994647            TELSİZ   \n",
       "2          Kadriye Gök Parkı  28.899872  40.990061          NURİPAŞA   \n",
       "3          Çukurbostan Parkı  28.933638  41.011341         ŞEHREMİNİ   \n",
       "4                Tutya Parkı  28.937360  41.009753       SEYYİD ÖMER   \n",
       "...                      ...        ...        ...               ...   \n",
       "3622       1415. Sok . Parkı  28.895567  41.104201              GAZİ   \n",
       "3623  Yedikule Surları Parkı  28.922778  40.993955          YEDİKULE   \n",
       "3624  Bağlarbaşı Çocuk Parkı  29.035293  41.024379        SELAMİ ALİ   \n",
       "3625                    Park  29.027359  41.085210              AKAT   \n",
       "3626                    Park  28.827317  41.007614        ÇOBANÇEŞME   \n",
       "\n",
       "       COUNTY_NAME  \n",
       "0       EYÜPSULTAN  \n",
       "1      ZEYTİNBURNU  \n",
       "2      ZEYTİNBURNU  \n",
       "3            FATİH  \n",
       "4            FATİH  \n",
       "...            ...  \n",
       "3622    SULTANGAZİ  \n",
       "3623         FATİH  \n",
       "3624       ÜSKÜDAR  \n",
       "3625      BEŞİKTAŞ  \n",
       "3626  BAHÇELİEVLER  \n",
       "\n",
       "[3627 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For natural gas usage NO2 and SO2 pollutants are important\n",
    "park_pd = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/park_location.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True\n",
    ")\n",
    "park_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3d7ab1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using sensor locations sensor_l, calculate the sum of distance from each waste facility to sensors\n",
    "park_dist = []\n",
    "for s_l in sensor_l ['İlçe']:\n",
    "    # Get current sensors location data\n",
    "    s_l_data = sensor_l [sensor_l ['İlçe'] == s_l]\n",
    "    sensor_lat = s_l_data ['Enlem'].values [0]\n",
    "    sensor_lon = s_l_data ['Boylam'].values [0]\n",
    "    close_park_count = 0\n",
    "    # Iterate all waste faciliies\n",
    "    for index, row in park_pd.iterrows():\n",
    "        waste_lat = row ['LATITUDE']\n",
    "        waste_lon = row ['LONGITUDE']\n",
    "        # Calculate distance\n",
    "        dist = coord_distance (sensor_lat, sensor_lon, waste_lat, waste_lon)\n",
    "        if dist < 2.5:\n",
    "            close_park_count += 1\n",
    "    \n",
    "    park_dist.append (close_park_count)\n",
    "      \n",
    "\n",
    "        \n",
    "# Calculate the adj matrix, the park number is inversly correlated with pollution!\n",
    "park_dist = np.asarray (park_dist)\n",
    "park_dist = 1 / park_dist\n",
    "park_dist = park_dist / park_dist.max ()\n",
    "\n",
    "park_adj = np.zeros ((len(park_dist), len (park_dist)))\n",
    "\n",
    "# We take the difference between consumptions\n",
    "for ix_one, park in enumerate (park_dist):\n",
    "    for ix_two, park in enumerate (park_dist):\n",
    "        if ix_one == ix_two:\n",
    "            park_adj [ix_one, ix_two] = 1\n",
    "        else:\n",
    "            park_adj [ix_one, ix_two] = np.abs (park_dist [ix_one] - park_dist [ix_two]) \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9265e009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/park_location_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(park_adj):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a63f7f",
   "metadata": {},
   "source": [
    "## Amount of Waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "53bac272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For natural gas usage NO2 and SO2 pollutants are important\n",
    "waste_amount_pd = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/amount_of_waste.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f88ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İlçe listesi\n",
    "sehirList = ['Fatih', 'Beşiktaş','Adalar','Fatih','Esenler','Kadıköy','Üsküdar','Kartal','Şişli','Ümraniye']\n",
    "\n",
    "# Preparing adjacency matrix for the sensor nodes\n",
    "waste_amount_matrix = np.zeros ((len (sehirList), len (sehirList)))\n",
    "\n",
    "# Create custom dictionary in order to create a new pandas dataframe for correlation\n",
    "waste_amount_dict = {}\n",
    "\n",
    "for data in waste_amount_pd['İlçe (Disticts)']:\n",
    "    if data in sehirList:\n",
    "        row = waste_amount_pd.loc[waste_amount_pd['İlçe (Disticts)'] == data]\n",
    "        waste_amount = row['2020'].values[0]\n",
    "        \n",
    "        waste_amount_dict [data] = waste_amount\n",
    "        \n",
    "final_dict = []        \n",
    "        \n",
    "for dist in sehirList:\n",
    "    final_dict.append( waste_amount_dict [dist])\n",
    "      \n",
    "final_dict = np.asarray (final_dict)\n",
    "final_dict = final_dict / final_dict.max ()\n",
    "\n",
    "# We take the difference between consumptions\n",
    "for ix_one, sehir in enumerate (sehirList):\n",
    "    for ix_two, sehir in enumerate (sehirList):\n",
    "        if ix_one == ix_two:\n",
    "            waste_amount_matrix [ix_one, ix_two] = 1\n",
    "        else:\n",
    "            waste_amount_matrix [ix_one, ix_two] = np.abs (final_dict [ix_one] - final_dict [ix_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a879592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/waste_amount_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(waste_amount_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74909318",
   "metadata": {},
   "source": [
    "## Population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b2a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_pd = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/population.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f3f51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İlçe listesi\n",
    "sehirList = ['Fatih', 'Beşiktaş','Adalar','Fatih','Esenler','Kadıköy','Üsküdar','Kartal','Şişli','Ümraniye']\n",
    "\n",
    "# Preparing adjacency matrix for the sensor nodes\n",
    "adj_matrix = np.zeros ((len (sehirList), len (sehirList)))\n",
    "\n",
    "# Create custom dictionary in order to create a new pandas dataframe for correlation\n",
    "adj_dict = {}\n",
    "\n",
    "for data in population_pd ['İlçe']:\n",
    "    if data in sehirList:\n",
    "        row = population_pd.loc[population_pd ['İlçe'] == data]\n",
    "        pop_count = row ['İlçe Nüfusu'].values[0]\n",
    "        \n",
    "        adj_dict [data] = pop_count\n",
    "        \n",
    "final_dict = []        \n",
    "        \n",
    "for dist in sehirList:\n",
    "    final_dict.append( adj_dict [dist])\n",
    "      \n",
    "final_dict = np.asarray (final_dict)\n",
    "final_dict = final_dict / final_dict.max ()\n",
    "\n",
    "# We take the difference between consumptions\n",
    "for ix_one, sehir in enumerate (sehirList):\n",
    "    for ix_two, sehir in enumerate (sehirList):\n",
    "        if ix_one == ix_two:\n",
    "            adj_matrix [ix_one, ix_two] = 1\n",
    "        else:\n",
    "            adj_matrix [ix_one, ix_two] = np.abs (final_dict [ix_one] - final_dict [ix_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6a7518c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/population_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(adj_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41686878",
   "metadata": {},
   "source": [
    "## Readers and Writers Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a99db5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_pd = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/2019-yl-ilce-bazl-okuma-yazma-bilen-bilmeyen-kii-says.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4071c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İlçe listesi\n",
    "sehirList = ['Fatih', 'Beşiktaş','Adalar','Fatih','Esenler','Kadıköy','Üsküdar','Kartal','Şişli','Ümraniye']\n",
    "\n",
    "# Preparing adjacency matrix for the sensor nodes\n",
    "adj_matrix = np.zeros ((len (sehirList), len (sehirList)))\n",
    "\n",
    "# Create custom dictionary in order to create a new pandas dataframe for correlation\n",
    "adj_dict = {}\n",
    "\n",
    "for data in writer_pd ['İlçeler']:\n",
    "    if data in sehirList:\n",
    "        row = writer_pd.loc[writer_pd ['İlçeler'] == data]\n",
    "        non_writers = row [' Okuma Yazma Bilmeyen'].values [0]\n",
    "        writers = row ['Okuma Yazma Bilen'].values [0]\n",
    "        \n",
    "        adj_dict [data] = (1.0 * non_writers) / (non_writers + writers)\n",
    "        \n",
    "final_dict = []        \n",
    "        \n",
    "for dist in sehirList:\n",
    "    final_dict.append( adj_dict [dist])\n",
    "      \n",
    "final_dict = np.asarray (final_dict)\n",
    "\n",
    "# We take the difference between consumptions\n",
    "for ix_one, sehir in enumerate (sehirList):\n",
    "    for ix_two, sehir in enumerate (sehirList):\n",
    "        if ix_one == ix_two:\n",
    "            adj_matrix [ix_one, ix_two] = 1\n",
    "        else:\n",
    "            adj_matrix [ix_one, ix_two] = np.abs (final_dict [ix_one] - final_dict [ix_two])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cc4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/non_writer_percentage_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(adj_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14593a",
   "metadata": {},
   "source": [
    "## Building Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23590ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>İlçe</th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adalar</td>\n",
       "      <td>6393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Arnavutköy</td>\n",
       "      <td>31941.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ataşehir</td>\n",
       "      <td>27583.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avcılar</td>\n",
       "      <td>26762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bağcılar</td>\n",
       "      <td>42439.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>833</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>834</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>837 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           İlçe    Total\n",
       "0        Adalar   6393.0\n",
       "1    Arnavutköy  31941.0\n",
       "2      Ataşehir  27583.0\n",
       "3       Avcılar  26762.0\n",
       "4      Bağcılar  42439.0\n",
       "..          ...      ...\n",
       "832         NaN      NaN\n",
       "833         NaN      NaN\n",
       "834         NaN      NaN\n",
       "835         NaN      NaN\n",
       "836         NaN      NaN\n",
       "\n",
       "[837 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_pd = pd.read_excel(\n",
    "     os.path.join(\"../datasets/adjacency/building_counts_2017.xlsx\"),\n",
    "     engine='openpyxl',\n",
    "     parse_dates=True,\n",
    "     sheet_name='Sum'\n",
    ")\n",
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f236be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İlçe listesi\n",
    "sehirList = ['Fatih', 'Beşiktaş','Adalar','Fatih','Esenler','Kadıköy','Üsküdar','Kartal','Şişli','Ümraniye']\n",
    "\n",
    "# Preparing adjacency matrix for the sensor nodes\n",
    "adj_matrix = np.zeros ((len (sehirList), len (sehirList)))\n",
    "\n",
    "# Create custom dictionary in order to create a new pandas dataframe for correlation\n",
    "adj_dict = {}\n",
    "\n",
    "for data in data_pd ['İlçe']:\n",
    "    if data in sehirList:\n",
    "        row = data_pd.loc[data_pd ['İlçe'] == data]\n",
    "        neighbors = row ['Total'].values [0]\n",
    "        adj_dict [data] = neighbors\n",
    "        \n",
    "final_dict = []        \n",
    "        \n",
    "for dist in sehirList:\n",
    "    final_dict.append( adj_dict [dist])\n",
    "      \n",
    "final_dict = np.asarray (final_dict)\n",
    "final_dict = final_dict / np.max (final_dict)\n",
    "\n",
    "# We take the difference between consumptions\n",
    "for ix_one, sehir in enumerate (sehirList):\n",
    "    for ix_two, sehir in enumerate (sehirList):\n",
    "        if ix_one == ix_two:\n",
    "            adj_matrix [ix_one, ix_two] = 1\n",
    "        else:\n",
    "            adj_matrix [ix_one, ix_two] = np.abs (final_dict [ix_one] - final_dict [ix_two])\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d87b9985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to excel\n",
    "workbook = xlsxwriter.Workbook('../datasets/training/neighbor_adj.xlsx')\n",
    "worksheet = workbook.add_worksheet()\n",
    "\n",
    "row = 0\n",
    "\n",
    "for col, data in enumerate(adj_matrix):\n",
    "    worksheet.write_column(row, col, data)\n",
    "\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373c4258",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
