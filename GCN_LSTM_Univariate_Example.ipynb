{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any number of GCN and LSTM layers can be added\n",
    "# Dropout and dense layers decrease the over-fitting problem\n",
    "# TODO: What can we change?\n",
    "# Method here is used for univariate (single-value) scalars\n",
    "# https://stellargraph.readthedocs.io/en/stable/demos/time-series/gcn-lstm-time-series.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All necessary imports here\n",
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input\n",
    "\n",
    "import stellargraph as sg\n",
    "\n",
    "from stellargraph.layer import GCN_LSTM\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of sensors: 207 \n",
      "No of timesteps: 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: ExperimentalWarning: METR_LA is experimental: tests and documentation missing (see: https://github.com/stellargraph/stellargraph/issues/1303). It may be difficult to use and may have major changes at any time.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Loading Datasets\n",
    "# A N by N adjacency matrix, which describes the distance relationship between the N sensors,\n",
    "# A N by T feature matrix, which describes the (f_1, .., f_T) speed records over T timesteps \n",
    "# for the N sensors.\n",
    "dataset = sg.datasets.METR_LA()\n",
    "speed_data, sensor_dist_adj = dataset.load()\n",
    "num_nodes, time_len = speed_data.shape\n",
    "print(\"No. of sensors:\", num_nodes, \"\\nNo of timesteps:\", time_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>773869</th>\n",
       "      <td>64.375</td>\n",
       "      <td>62.666667</td>\n",
       "      <td>64.00</td>\n",
       "      <td>61.777778</td>\n",
       "      <td>59.555556</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>66.500</td>\n",
       "      <td>63.625</td>\n",
       "      <td>68.750</td>\n",
       "      <td>63.500</td>\n",
       "      <td>...</td>\n",
       "      <td>64.625</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>63.125</td>\n",
       "      <td>62.875</td>\n",
       "      <td>68.375</td>\n",
       "      <td>65.555556</td>\n",
       "      <td>66.625</td>\n",
       "      <td>66.375</td>\n",
       "      <td>64.666667</td>\n",
       "      <td>66.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767541</th>\n",
       "      <td>67.625</td>\n",
       "      <td>68.555556</td>\n",
       "      <td>63.75</td>\n",
       "      <td>65.500000</td>\n",
       "      <td>67.250000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>63.875</td>\n",
       "      <td>67.250</td>\n",
       "      <td>65.250</td>\n",
       "      <td>61.500</td>\n",
       "      <td>...</td>\n",
       "      <td>67.125</td>\n",
       "      <td>66.555556</td>\n",
       "      <td>68.125</td>\n",
       "      <td>67.125</td>\n",
       "      <td>67.000</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>66.625</td>\n",
       "      <td>66.375</td>\n",
       "      <td>66.555556</td>\n",
       "      <td>67.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767542</th>\n",
       "      <td>67.125</td>\n",
       "      <td>65.444444</td>\n",
       "      <td>60.00</td>\n",
       "      <td>62.555556</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>67.666667</td>\n",
       "      <td>67.875</td>\n",
       "      <td>63.250</td>\n",
       "      <td>63.500</td>\n",
       "      <td>62.500</td>\n",
       "      <td>...</td>\n",
       "      <td>66.500</td>\n",
       "      <td>66.222222</td>\n",
       "      <td>67.625</td>\n",
       "      <td>68.625</td>\n",
       "      <td>68.500</td>\n",
       "      <td>64.111111</td>\n",
       "      <td>65.750</td>\n",
       "      <td>63.750</td>\n",
       "      <td>66.888889</td>\n",
       "      <td>66.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717447</th>\n",
       "      <td>61.500</td>\n",
       "      <td>62.444444</td>\n",
       "      <td>59.00</td>\n",
       "      <td>59.888889</td>\n",
       "      <td>60.777778</td>\n",
       "      <td>61.666667</td>\n",
       "      <td>62.375</td>\n",
       "      <td>60.500</td>\n",
       "      <td>63.000</td>\n",
       "      <td>58.125</td>\n",
       "      <td>...</td>\n",
       "      <td>55.625</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>58.750</td>\n",
       "      <td>60.625</td>\n",
       "      <td>61.375</td>\n",
       "      <td>61.111111</td>\n",
       "      <td>64.750</td>\n",
       "      <td>63.750</td>\n",
       "      <td>59.888889</td>\n",
       "      <td>59.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717446</th>\n",
       "      <td>66.875</td>\n",
       "      <td>64.444444</td>\n",
       "      <td>66.50</td>\n",
       "      <td>66.777778</td>\n",
       "      <td>67.055556</td>\n",
       "      <td>67.333333</td>\n",
       "      <td>64.375</td>\n",
       "      <td>57.375</td>\n",
       "      <td>65.125</td>\n",
       "      <td>66.625</td>\n",
       "      <td>...</td>\n",
       "      <td>65.625</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>66.625</td>\n",
       "      <td>67.125</td>\n",
       "      <td>64.500</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>67.375</td>\n",
       "      <td>67.625</td>\n",
       "      <td>65.111111</td>\n",
       "      <td>64.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2016 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1      2          3          4          5       6     \\\n",
       "773869  64.375  62.666667  64.00  61.777778  59.555556  57.333333  66.500   \n",
       "767541  67.625  68.555556  63.75  65.500000  67.250000  69.000000  63.875   \n",
       "767542  67.125  65.444444  60.00  62.555556  65.111111  67.666667  67.875   \n",
       "717447  61.500  62.444444  59.00  59.888889  60.777778  61.666667  62.375   \n",
       "717446  66.875  64.444444  66.50  66.777778  67.055556  67.333333  64.375   \n",
       "\n",
       "          7       8       9     ...    2006       2007    2008    2009  \\\n",
       "773869  63.625  68.750  63.500  ...  64.625  65.444444  63.125  62.875   \n",
       "767541  67.250  65.250  61.500  ...  67.125  66.555556  68.125  67.125   \n",
       "767542  63.250  63.500  62.500  ...  66.500  66.222222  67.625  68.625   \n",
       "717447  60.500  63.000  58.125  ...  55.625  61.000000  58.750  60.625   \n",
       "717446  57.375  65.125  66.625  ...  65.625  58.000000  66.625  67.125   \n",
       "\n",
       "          2010       2011    2012    2013       2014    2015  \n",
       "773869  68.375  65.555556  66.625  66.375  64.666667  66.000  \n",
       "767541  67.000  65.111111  66.625  66.375  66.555556  67.125  \n",
       "767542  68.500  64.111111  65.750  63.750  66.888889  66.375  \n",
       "717447  61.375  61.111111  64.750  63.750  59.888889  59.250  \n",
       "717446  64.500  66.000000  67.375  67.625  65.111111  64.250  \n",
       "\n",
       "[5 rows x 2016 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train - Test split\n",
    "train_rate = 0.8\n",
    "def train_test_split(data, train_portion):\n",
    "    time_len = data.shape[1]\n",
    "    train_size = int(time_len * train_portion)\n",
    "    train_data = np.array(data.iloc[:, :train_size])\n",
    "    test_data = np.array(data.iloc[:, train_size:])\n",
    "    return train_data, test_data\n",
    "\n",
    "# Scaling\n",
    "def scale_data(train_data, test_data):\n",
    "    max_speed = train_data.max()\n",
    "    min_speed = train_data.min()\n",
    "    train_scaled = (train_data - min_speed) / (max_speed - min_speed)\n",
    "    test_scaled = (test_data - min_speed) / (max_speed - min_speed)\n",
    "    return train_scaled, test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:  (207, 1612)\n",
      "Test data:  (207, 404)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(speed_data, train_rate)\n",
    "print(\"Train data: \", train_data.shape)\n",
    "print(\"Test data: \", test_data.shape)\n",
    "train_scaled, test_scaled = scale_data(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This value represents the past feature window size\n",
    "seq_len = 10\n",
    "# This value represents how far ahead we want to predict\n",
    "pre_len = 12\n",
    "# Data for LSTM is prepared using the Sliding Window approach\n",
    "# Sliding windows are applied for each sensor\n",
    "def sequence_data_preparation(seq_len, pre_len, train_data, test_data):\n",
    "    trainX, trainY, testX, testY = [], [], [], []\n",
    "\n",
    "    for i in range(train_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        a = train_data[:, i : i + seq_len + pre_len]\n",
    "        trainX.append(a[:, :seq_len])\n",
    "        trainY.append(a[:, -1])\n",
    "\n",
    "    for i in range(test_data.shape[1] - int(seq_len + pre_len - 1)):\n",
    "        b = test_data[:, i : i + seq_len + pre_len]\n",
    "        testX.append(b[:, :seq_len])\n",
    "        testY.append(b[:, -1])\n",
    "\n",
    "    trainX = np.array(trainX)\n",
    "    trainY = np.array(trainY)\n",
    "    testX = np.array(testX)\n",
    "    testY = np.array(testY)\n",
    "\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1591, 207, 10)\n",
      "(1591, 207)\n",
      "(383, 207, 10)\n",
      "(383, 207)\n"
     ]
    }
   ],
   "source": [
    "# We expect to see features of seq_len window size for each sensor \n",
    "# and labels as the pre_len ahead output\n",
    "trainX, trainY, testX, testY = sequence_data_preparation(\n",
    "    seq_len, pre_len, train_scaled, test_scaled\n",
    ")\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the GCN LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\halit u\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: ExperimentalWarning: GCN_LSTM is experimental: Lack of unit tests and code refinement (see: https://github.com/stellargraph/stellargraph/issues/1132, https://github.com/stellargraph/stellargraph/issues/1526, https://github.com/stellargraph/stellargraph/issues/1564). It may be difficult to use and may have major changes at any time.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "## GCN and LSTM Model\n",
    "gc_layer_sizes = [16, 10]\n",
    "gc_activations = [\"relu\", \"relu\"]\n",
    "lstm_layer_sizes = [200, 200]\n",
    "lstm_activations = [\"tanh\", \"tanh\"]\n",
    "\n",
    "gcn_lstm = GCN_LSTM(\n",
    "    seq_len=seq_len,\n",
    "    adj=sensor_dist_adj,\n",
    "    gc_layer_sizes=gc_layer_sizes,\n",
    "    gc_activations=gc_activations,\n",
    "    lstm_layer_sizes=lstm_layer_sizes,\n",
    "    lstm_activations=lstm_activations,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_input, x_output = gcn_lstm.in_out_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=x_input, outputs=x_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = \"adam\"\n",
    "loss = \"mae\"\n",
    "metrics = [\"mse\"]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 60\n",
    "shuffle = True\n",
    "verbose = 0\n",
    "\n",
    "history = model.fit(\n",
    "    trainX,\n",
    "    trainY,\n",
    "    epochs=n_epochs,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=shuffle,\n",
    "    verbose=verbose,\n",
    "    validation_data=(testX, testY),\n",
    ")\n",
    "\n",
    "print(\"Execution time: %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Train loss: \",\n",
    "    history.history[\"loss\"][-1],\n",
    "    \"\\nTest loss:\",\n",
    "    history.history[\"val_loss\"][-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sg.utils.plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual prediction\n",
    "ythat = model.predict(trainX)\n",
    "yhat = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to rescale the predictions because we applied normalization in the first place\n",
    "## Rescale values\n",
    "max_speed = train_data.max()\n",
    "min_speed = train_data.min()\n",
    "\n",
    "## actual train and test values\n",
    "train_rescref = np.array(trainY * max_speed)\n",
    "test_rescref = np.array(testY * max_speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale model predicted values\n",
    "train_rescpred = np.array((ythat) * max_speed)\n",
    "test_rescpred = np.array((yhat) * max_speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measuring Performance\n",
    "## Naive prediction benchmark (using previous observed value)\n",
    "\n",
    "testnpred = np.array(testX)[\n",
    "    :, :, -1\n",
    "]  # picking the last speed of the 10 sequence for each segment in each sample\n",
    "testnpredc = (testnpred) * max_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance measures\n",
    "\n",
    "seg_mael = []\n",
    "seg_masel = []\n",
    "seg_nmael = []\n",
    "\n",
    "for j in range(testX.shape[-1]):\n",
    "\n",
    "    seg_mael.append(\n",
    "        np.mean(np.abs(test_rescref.T[j] - test_rescpred.T[j]))\n",
    "    )  # Mean Absolute Error for NN\n",
    "    seg_nmael.append(\n",
    "        np.mean(np.abs(test_rescref.T[j] - testnpredc.T[j]))\n",
    "    )  # Mean Absolute Error for naive prediction\n",
    "    if seg_nmael[-1] != 0:\n",
    "        seg_masel.append(\n",
    "            seg_mael[-1] / seg_nmael[-1]\n",
    "        )  # Ratio of the two: Mean Absolute Scaled Error\n",
    "    else:\n",
    "        seg_masel.append(np.NaN)\n",
    "\n",
    "print(\"Total (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "print(\"Total (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "print(\n",
    "    \"Total (ave) MASE for per-segment NN/naive MAE: \"\n",
    "    + str(np.nanmean(np.array(seg_masel)))\n",
    ")\n",
    "print(\n",
    "    \"...note that MASE<1 (for a given segment) means that the NN prediction is better than the naive prediction.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot violin plot of MAE for naive and NN predictions\n",
    "fig, ax = plt.subplots()\n",
    "# xl = minsl\n",
    "\n",
    "ax.violinplot(\n",
    "    list(seg_mael), showmeans=True, showmedians=False, showextrema=False, widths=1.0\n",
    ")\n",
    "\n",
    "ax.violinplot(\n",
    "    list(seg_nmael), showmeans=True, showmedians=False, showextrema=False, widths=1.0\n",
    ")\n",
    "\n",
    "line1 = mlines.Line2D([], [], label=\"NN\")\n",
    "line2 = mlines.Line2D([], [], color=\"C1\", label=\"Instantaneous\")\n",
    "\n",
    "ax.set_xlabel(\"Scaled distribution amplitude (after Gaussian convolution)\")\n",
    "ax.set_ylabel(\"Mean Absolute Error\")\n",
    "ax.set_title(\"Distribution over segments: NN pred (blue) and naive pred (orange)\")\n",
    "plt.legend(handles=(line1, line2), title=\"Prediction Model\", loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##all test result visualization\n",
    "fig1 = plt.figure(figsize=(15, 8))\n",
    "#    ax1 = fig1.add_subplot(1,1,1)\n",
    "a_pred = test_rescpred[:, 100]\n",
    "a_true = test_rescref[:, 100]\n",
    "plt.plot(a_pred, \"r-\", label=\"prediction\")\n",
    "plt.plot(a_true, \"b-\", label=\"true\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"speed\")\n",
    "plt.legend(loc=\"best\", fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output all results to a sub-directory\n",
    "import datetime\n",
    "import pathlib\n",
    "import os\n",
    "path = pathlib.Path().absolute()\n",
    "time = datetime.datetime.now().strftime(\"%m-%d-%Y %H-%M-%S\")\n",
    "result_dir = os.path.join (path, \"experiments\", time)\n",
    "os.mkdir (result_dir)\n",
    "\n",
    "# Create the result txt file\n",
    "f = open(result_dir + \"/\" + \"summary.txt\", \"w\")\n",
    "\n",
    "# Write experiment parameters\n",
    "f.write (\"---------Experiment Parameters--------\")\n",
    "f.write (\"\\nPast feature window: \" + str(seq_len))\n",
    "f.write (\"\\nFuture feature window: \" + str(pre_len))\n",
    "f.write (\"\\nTrain-Test split rate: \" + str(train_rate))\n",
    "f.write (\"\\nNumber of epochs: \" + str(n_epochs))\n",
    "f.write (\"\\nBatch size: \" + str(batch_size))\n",
    "f.write (\"\\nShuffle during training: \" + str(shuffle))\n",
    "f.write (\"\\nVerbose: \" + str(verbose))\n",
    "f.write (\"\\n---------------------------\")\n",
    "\n",
    "# Write dataset details\n",
    "f.write (\"\\n---------Dataset Details--------\")\n",
    "num_nodes, time_len = speed_data.shape\n",
    "f.write (\"\\nNo. of sensors:\" + str(num_nodes) + \"\\nNo of timesteps:\" + str(time_len))\n",
    "f.write (\"\\nTrain data: \" + str(train_data.shape))\n",
    "f.write (\"\\nTest data: \" + str(test_data.shape))\n",
    "\n",
    "f.write (\"\\nTrain data X: \" + str(trainX.shape))\n",
    "f.write (\"\\nTrain data Y: \" + str(trainY.shape))\n",
    "f.write (\"\\nTest data X: \" + str(testX.shape))\n",
    "f.write (\"\\nTest data Y: \" + str(testY.shape))\n",
    "f.write (\"\\n---------------------------\")\n",
    "\n",
    "# Write model details\n",
    "f.write (\"\\n---------Model Details--------\")\n",
    "f.write (\"\\nGCN layer sizes: \" + str(gc_layer_sizes))\n",
    "f.write (\"\\nGCN activation functions: \" + str(gc_activations))\n",
    "f.write (\"\\nLSTM layer sizes: \" + str(lstm_layer_sizes))\n",
    "f.write (\"\\nLSTM activation functions: \" + str(lstm_activations))\n",
    "f.write (\"\\nOptimizer: \" + optimizer)\n",
    "f.write (\"\\nLoss function: \" + loss)\n",
    "f.write (\"\\nResult Metrics: \" + str(metrics))\n",
    "f.write (\"\\n---------------------------\")\n",
    "\n",
    "# Write results\n",
    "f.write (\"\\nLast Train loss: \" + str(history.history[\"loss\"][-1]) + \n",
    "         \", Last Test loss:\" + str(history.history[\"val_loss\"][-1]))\n",
    "f.write (\"\\nTotal (ave) MAE for NN: \" + str(np.mean(np.array(seg_mael))))\n",
    "f.write (\"\\nTotal (ave) MAE for naive prediction: \" + str(np.mean(np.array(seg_nmael))))\n",
    "f.write (\"Total (ave) MASE for per-segment NN/naive MAE: \"\n",
    "    + str(np.nanmean(np.array(seg_masel))))\n",
    "f.write (\"...note that MASE<1 (for a given segment) means that the\" + \n",
    "         \"NN prediction is better than the naive prediction.\")\n",
    "\n",
    "# Write visual outputs\n",
    "fig.savefig (result_dir + \"/\" + \"MAE-vs-Naive.png\")\n",
    "fig1.savefig (result_dir + \"/\" + \"prediction-accuracy.png\")\n",
    "\n",
    "f.close ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
